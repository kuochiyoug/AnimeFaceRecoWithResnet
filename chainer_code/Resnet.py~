#Chainer V2 code! Check carefully!
import math
import numpy as np
import chainer
import chainer.links as L
import chainer.functions as F
from chainer import cuda
from chainer import optimizers
from chainer import training
from chainer.training import extensions
from chainer.cuda import cupy as xp
from chainer.dataset import convert
import six
from six.moves import queue
from chainer import serializers
#
from chainer.datasets import tuple_dataset
from PIL import Image
import numpy as np
import glob,os


# In[2]:


#Define small blocks in residual net      
class BottleNeckA(chainer.Chain):
    def __init__(self, in_size, ch, out_size, stride=2):
        w = math.sqrt(2)
        super(BottleNeckA, self).__init__(
            conv1=L.Convolution2D(in_size, ch, 1, stride, 0, w, initial_bias=False),
            bn1=L.BatchNormalization(ch),
            conv2=L.Convolution2D(ch, ch, 3, 1, 1, w, initial_bias=False),
            bn2=L.BatchNormalization(ch),
            conv3=L.Convolution2D(ch, out_size, 1, 1, 0, w, initial_bias=False),
            bn3=L.BatchNormalization(out_size),

            conv4=L.Convolution2D(in_size, out_size, 1, stride, 0, w, initial_bias=False),
            bn4=L.BatchNormalization(out_size),
        )

    def __call__(self, x, train):
        h1 = F.relu(self.bn1(self.conv1(x)))
        h1 = F.relu(self.bn2(self.conv2(h1)))
        h1 = self.bn3(self.conv3(h1))
        h2 = self.bn4(self.conv4(x))

        return F.relu(h1 + h2)

        
class BottleNeckB(chainer.Chain):
    def __init__(self, in_size, ch):
        w = math.sqrt(2)
        super(BottleNeckB, self).__init__(
            conv1=L.Convolution2D(in_size, ch, 1, 1, 0, w, initial_bias=False),
            bn1=L.BatchNormalization(ch),
            conv2=L.Convolution2D(ch, ch, 3, 1, 1, w, initial_bias=False),
            bn2=L.BatchNormalization(ch),
            conv3=L.Convolution2D(ch, in_size, 1, 1, 0, w, initial_bias=False),
            bn3=L.BatchNormalization(in_size),
        )

    def __call__(self, x, train):
        h = F.relu(self.bn1(self.conv1(x)))
        h = F.relu(self.bn2(self.conv2(h)))
        h = self.bn3(self.conv3(h))

        return F.relu(h + x)

    
class Block(chainer.Chain):
    def __init__(self, layer, in_size, ch, out_size, stride=2):
        super(Block, self).__init__()
        links = [('a', BottleNeckA(in_size, ch, out_size, stride))]
        for i in range(layer-1):
            links += [('b{}'.format(i+1), BottleNeckB(out_size, ch))]

        for l in links:
            self.add_link(*l)
        self.forward = links

    def __call__(self, x, train):
        for name, _ in self.forward:
            f = getattr(self, name)
            x = f(x, train)

        return x


# In[3]:


class ResNet(chainer.Chain):

    insize = 224

    def __init__(self):
        w = math.sqrt(2)
        super(ResNet, self).__init__(
            conv1=L.Convolution2D(3, 64, 7, 2, 3, w, initial_bias=False),
            bn1=L.BatchNormalization(64),
            res2=Block(layer=3, in_size=64, ch=64, out_size=256, stride=1),
            res3=Block(4, 256, 128, 512, 2),
            res4=Block(23, 512, 256, 1024, 2),
            res5=Block(3, 1024, 512, 2048, 2),
            fc=L.Linear(2048, 1000)
        )
        self.train = True

    def clear(self):
        self.loss = None
        self.accuracy = None

    def __call__(self, x, t):
        self.clear()
        h = self.bn1(self.conv1(x))
        h = F.max_pooling_2d(F.relu(h), 3, stride=2)
        h = self.res2(h, self.train)
        h = self.res3(h, self.train)
        h = self.res4(h, self.train)
        h = self.res5(h, self.train)
        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.fc(h)

        if self.train:
            self.loss = F.softmax_cross_entropy(h, t)
            self.accuracy = F.accuracy(h, t)
            return self.loss
        else:
            return h


# In[4]:


def data_manage_animefacedata_gpu(data_path="/home/koma/dataset/animeface-character-dataset/thumb/"):
    #Data path setup
   
    #data_path = "/home/koma/dataset/animeface-character-dataset/thumb/"
    #test_data_path = "/data/test/"
    
    #folders = glob.glob(data_path+"*")
    folders = sorted(os.listdir(data_path))
    cats = []#Categorys list
    all_data = []
    for folder in folders:
        if os.path.isfile(data_path + folder + "/" + "ignore"):
            print("Folder "+ folder + "is ignored!")
            continue
        else:
            cats.append(folder)
            label = folder
            img_filelist = glob.glob(data_path + folder + "/"+"*.png")
            for imgfile in img_filelist:
                all_data.append([imgfile,label])
    print("labels="+str(len(cats)))
    
    all_data = np.random.permutation(all_data)
            
    imageData = []
    labelData = []
    for PathAndLabel in all_data:
        img = Image.open(PathAndLabel[0])
        img = img.resize((160,160))
        label_id = cats.index(PathAndLabel[1])
        #print PathAndLabel[1]
        img = np.asarray(np.float32(img))
        if img.shape[2] != 3:
            continue
        img = np.reshape((img),(3,160,160))
        
        imageData.append(img)
        labelData.append(np.int32(label_id))
    
    threshold = np.int32(len(imageData)/8*7)
    train = tuple_dataset.TupleDataset(xp.asarry(imageData[0:threshold]), xp.asarry(labelData[0:threshold]))
    test  = tuple_dataset.TupleDataset(xp.asarry(imageData[threshold:]),  xp.asarry(labelData[threshold:]))
    return train,test
    
    
    #train_batch = np.ndarray((50,3,160,160),dtype=np.float32)
    #test_batch = np.ndarray((50,3,160,160),dtype=np.float32)
    


# In[5]:


def data_manage_animefacedata(data_path="/home/koma/dataset/animeface-character-dataset/thumb/",in_size=224):
    #Data path setup
   
    #data_path = "/home/koma/dataset/animeface-character-dataset/thumb/"
    #test_data_path = "/data/test/"
    
    #folders = glob.glob(data_path+"*")
    folders = sorted(os.listdir(data_path))
    cats = []#Categorys list
    all_data = []
    for folder in folders:
        if os.path.isfile(data_path + folder + "/" + "ignore"):
            #print("Folder "+ folder + "is ignored!")
            continue
        else:
            cats.append(folder)
            label = folder
            img_filelist = glob.glob(data_path + folder + "/"+"*.png")
            for imgfile in img_filelist:
                all_data.append([imgfile,label])
    print("labels="+str(len(cats)))
    
    all_data = np.random.permutation(all_data) #Random the rank
            
    imageData = []
    labelData = []
    for PathAndLabel in all_data:
        img = Image.open(PathAndLabel[0])
        img = img.resize((in_size,in_size))
        label_id = cats.index(PathAndLabel[1])
        #print PathAndLabel[1]
        img = np.asarray(np.float32(img))
        if img.shape[2] != 3:
            continue
        img = np.reshape(img,(3,in_size,in_size))
        imageData.append(img)
        labelData.append(np.int32(label_id))
    
    threshold = np.int32(len(imageData)/8*7)
    train = tuple_dataset.TupleDataset(imageData[0:threshold], labelData[0:threshold])
    test  = tuple_dataset.TupleDataset(imageData[threshold:],  labelData[threshold:])
    return train,test
    
    
    #train_batch = np.ndarray((50,3,160,160),dtype=np.float32)
    #test_batch = np.ndarray((50,3,160,160),dtype=np.float32)
    


# In[6]:


if __name__ == "__main__":
    
    #Check cuda environment and setup models
    cuda.check_cuda_available()
    from chainer.cuda import cupy as xp
    
    model = ResNet()
    cuda.get_device(0).use()
    model.to_gpu(0)
    print("Model Ready with GPU")


# In[7]:


#Setup optimizer
optimizer = optimizers.Adam(alpha=0.0002, beta1=0.5)
optimizer.setup(model)
optimizer.add_hook(chainer.optimizer.WeightDecay(0.00001))
print("Optimizer Sets!")


# In[ ]:


#Setup train data and test data
train, test = data_manage_animefacedata()
train_iter = chainer.iterators.SerialIterator(train,20)
test_iter = chainer.iterators.SerialIterator(test,10,repeat=False,shuffle=False)
print("Data Loaded!")


# In[ ]:


sum_accuracy = 0
sum_loss = 0
epochnum = 10

train_count = len(train)
test_count = len(test)
#while(train_iter.epoch < args.epoch):
while(train_iter.epoch < epochnum):
    batch = train_iter.next()
    #print len(batch)
    print('train mean loss: {}, accuracy: {}'.format(
            sum_loss / train_count, sum_accuracy / train_count))
    # Reduce learning rate by 0.5 every 25 epochs.
    #if train_iter.epoch % 25 == 0 and train_iter.is_new_epoch:
    #   optimizer.lr *= 0.5
    #   print('Reducing learning rate to: ', optimizer.lr)

    #x_array, t_array = convert.concat_examples(batch, args.gpu)
    x_array, t_array = convert.concat_examples(batch, 0)
    x = chainer.Variable(x_array)
    t = chainer.Variable(t_array)
    optimizer.update(model, x, t)
    sum_loss += float(model.loss.data) * len(t.data)
    sum_accuracy += float(model.accuracy.data) * len(t.data)
    
    if train_iter.is_new_epoch:
        print('epoch: ', train_iter.epoch)
        print('train mean loss: {}, accuracy: {}'.format(
            sum_loss / train_count, sum_accuracy / train_count))
        # evaluation
        sum_accuracy = 0
        sum_loss = 0
        #model.predictor.train = False
        for batch in test_iter:
            x_array, t_array = convert.concat_examples(batch, 0)
            x = chainer.Variable(x_array)
            t = chainer.Variable(t_array)
            loss = model(x, t)
            sum_loss += float(loss.data) * len(t.data)
            sum_accuracy += float(model.accuracy.data) * len(t.data)

        test_iter.reset()
        #model.predictor.train = True
        print('test mean  loss: {}, accuracy: {}'.format(
            sum_loss / test_count, sum_accuracy / test_count))
        sum_accuracy = 0
        sum_loss = 0

        # Save the model and the optimizer
        print('save the model')
        serializers.save_npz('mlp.model', model)
        print('save the optimizer')
        serializers.save_npz('mlp.state', optimizer)

